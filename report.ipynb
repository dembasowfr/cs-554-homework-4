{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1166313a",
   "metadata": {},
   "source": [
    "# CS454-554 Homework 4 Report: KMNIST Classification\n",
    "\n",
    "**Due:** May 14, 2025, 23:00\n",
    "**Author:** DEMBA SOW\n",
    "\n",
    "---\n",
    "## 1. Abstract\n",
    "This report compares multiple CNN architectures on the KMNIST handwritten character classification task. Models include a baseline Linear classifier, MLP, Simple CNN, and advanced variants: CNN-A (BatchNorm + Dropout), CNN-B (Global Pooling), and CNN-C (Residual Blocks). All models are implemented using PyTorch. CNN-A achieved the best performance with 95.7% test accuracy. Experiments highlight the benefits of normalization, residual connections, and dropout for improving generalization in convolutional neural networks.\n",
    "\n",
    "2. **Introduction**\n",
    "3. **Baseline Models**\n",
    "\n",
    "   * Linear model\n",
    "   * Multilayer Perceptron (40 hidden units)\n",
    "   * Simple CNN\n",
    "4. **Extended CNN Variants**\n",
    "   * CNN‑A (Deeper + BatchNorm + Dropout)\n",
    "   * CNN‑B (Wider Filters + Global Pooling)\n",
    "   * CNN‑C (Residual Connections)\n",
    "\n",
    "Each section includes:\n",
    "\n",
    "* Model architecture description\n",
    "* Placeholder for code snippet insertion\n",
    "* Training & validation loss/accuracy curves\n",
    "* Numerical results table\n",
    "* Per‑model analysis\n",
    "\n",
    "5. **Hyperparameter Tuning & Early Stopping**\n",
    "6. **Comparative Analysis & Discussion**\n",
    "7. **Submission Details**\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Introduction\n",
    "\n",
    "The objective of this assignment is to implement and compare six neural network models of increasing complexity on the KMNIST dataset (10-class, 28×28 grayscale Japanese characters) using PyTorch. We evaluate:\n",
    "\n",
    "\n",
    "## 3. Environment & Data Loading\n",
    "\n",
    "**Description:** Setup device, transforms, datasets, data loaders, and training/evaluation utilities.\n",
    "\n",
    "**Code Placeholder:**\n",
    "\n",
    "```python\n",
    "%pip install -r requirements.txt\n",
    "\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from torchvision.datasets import KMNIST\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "LR = 0.001\n",
    "EPOCHS_BASELINE = 20\n",
    "EPOCHS_EXTENDED = 50\n",
    "PATIENCE = 3\n",
    "\n",
    "# Transforms & DataLoaders\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "train_ds = KMNIST(root='./data', train=True,  transform=transform, download=True)\n",
    "test_ds  = KMNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,        # <— parallel data loading\n",
    "    pin_memory=True       # <— faster transfers to GPU\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "**Analysis:** Normalization to mean=0.5, std=0.5 stabilizes training; batch size 64 offers a balance of gradient noise and efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Baseline Models\n",
    "\n",
    "### 4.1 Linear Model\n",
    "\n",
    "* **Architecture:** Single fully‑connected layer (784→10)\n",
    "* **Code Placeholder:**\n",
    "```python\n",
    "# LinearModel class definition\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(28*28, 10)\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "```\n",
    "\n",
    "* **Training Setup:** Adam (lr=1e-3), CrossEntropyLoss, epochs=20, batch=64\n",
    "\n",
    "* **Learning Curves:** \n",
    "![Linear Accuracy & Loss Curves](./output/linear-model.png)\n",
    "\n",
    "\n",
    "* **Results:**\n",
    "\n",
    "  | Epoch | Train Loss | Test Loss | Train Acc | Test Acc |\n",
    "  | :---: | :--------: | :-------: | :-------: | :------: |\n",
    "  |   20  |    0.567   |   1.059   |   82.9%   |   68.8%  |\n",
    "* **Analysis:** Underfits due to limited capacity; high bias and \\~14% train–test gap.\n",
    "\n",
    "### 4.2 Multilayer Perceptron (MLP40)\n",
    "\n",
    "* **Architecture:** Flatten → FC(784→40) → ReLU → FC(40→10)\n",
    "* **Code Placeholder:**\n",
    "\n",
    "```python\n",
    "# MLP40 class definition\n",
    "class MLP40(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28*28, 40),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(40, 10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "```\n",
    "\n",
    "* **Training Setup:** same hyperparameters as Linear\n",
    "* **Learning Curves:** \n",
    "\n",
    "![MLP40 Accuracy & Loss Curves](./output/mlp-40-model.png)\n",
    "\n",
    "* **Results:**\n",
    "\n",
    "  | Epoch | Train Loss | Test Loss | Train Acc | Test Acc |\n",
    "  | :---: | :--------: | :-------: | :-------: | :------: |\n",
    "  |   20  |    0.006   |   0.450   |   99.8%   |   94.8%  |\n",
    "* **Analysis:** Achieves high capacity but overfits (\\~5% gap); ignores spatial structure.\n",
    "\n",
    "### 4.3 Simple CNN\n",
    "\n",
    "* **Architecture:**\n",
    "\n",
    "  1. Conv(1→16,3) → ReLU → MaxPool(2)\n",
    "  2. Conv(16→32,3) → ReLU → MaxPool(2)\n",
    "  3. FC(32×7×7→64) → ReLU → FC(64→10)\n",
    "* **Code Placeholder:**\n",
    "\n",
    "```python\n",
    "# SimpleCNN class definition\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),  # 16×14×14\n",
    "            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2)  # 32×7×7\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32*7*7, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return self.classifier(x)\n",
    "```\n",
    "\n",
    "* **Training Setup:** same hyperparameters\n",
    "* **Learning Curves:** \n",
    "![Simple CNN Accuracy & Loss Curves](./output/simple-cnn-model.png)\n",
    "\n",
    "* **Results:**\n",
    "\n",
    "  | Epoch | Train Loss | Test Loss | Train Acc | Test Acc |\n",
    "  | :---: | :--------: | :-------: | :-------: | :------: |\n",
    "  |   20  |    0.001   |   0.527   |   99.9%   |   94.6%  |\n",
    "* **Analysis:** Spatial inductive bias yields strong generalization, matching MLP40 with less overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Extended CNN Variants\n",
    "\n",
    "All three models trained with early stopping (patience=3), lr=5e-4, batch size=128, max epochs=50.\n",
    "\n",
    "### 5.1 CNN‑A (Deep + BN + Dropout)\n",
    "\n",
    "* **Architecture Description:** Two conv blocks with BatchNorm, ReLU, pooling, dropout; FC(128)+dropout\n",
    "* **Code Placeholder:**\n",
    "\n",
    "```python\n",
    "# CNN_A class definition\n",
    "class CNN_A(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1,32,3,padding=1), nn.BatchNorm2d(32), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32,64,3,padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
    "            nn.MaxPool2d(2), nn.Dropout(0.25)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*7*7,128), nn.ReLU(), nn.Dropout(0.5),\n",
    "            nn.Linear(128,10)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x = self.features(x)\n",
    "        return self.classifier(x)\n",
    "```\n",
    "\n",
    "* **Learning Curves:** \n",
    "\n",
    "![CNN-A Loss & Accuracy Curves](./output/cnn_a-model.png)\n",
    "\n",
    "* **Results:** Stop epoch \\~18, Test Acc = 95.68%\n",
    "* **Analysis:** BatchNorm accelerates convergence; dropout controls overfitting; balanced capacity.\n",
    "\n",
    "### 5.2 CNN‑B (Wide Filters + Global Pool)\n",
    "\n",
    "* **Architecture Description:** Two 5×5 conv layers, one 3×3 conv, global average pooling\n",
    "* **Code Placeholder:**\n",
    "\n",
    "```python\n",
    "# CNN_B class definition\n",
    "class CNN_B(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1,16,5,padding=2), nn.ReLU(),\n",
    "            nn.Conv2d(16,32,5,padding=2), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32,64,3,padding=1), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "        self.fc = nn.Linear(64,10)\n",
    "    def forward(self,x):\n",
    "        x = self.features(x).view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "```\n",
    "\n",
    "* **Learning Curves:** \n",
    "![CNN-B Loss & Accuracy Curves](./output/cnn_b-model.png)\n",
    "\n",
    "* **Results:** Stop epoch \\~23, Test Acc = 84.02%\n",
    "* **Analysis:** Lacks normalization; over-smoothing by global pooling; lower capacity.\n",
    "\n",
    "### 5.3 CNN‑C (Residual Blocks)\n",
    "\n",
    "* **Architecture Description:** Two ResidualBlocks + pooling; FC(64) → output\n",
    "* **Code Placeholder:**\n",
    "\n",
    "```python\n",
    "# ResidualBlock and CNN_C class definitions\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch,out_ch,3,padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(out_ch,out_ch,3,padding=1)\n",
    "        )\n",
    "        self.skip = nn.Conv2d(in_ch,out_ch,1) if in_ch!=out_ch else nn.Identity()\n",
    "    def forward(self,x):\n",
    "        return nn.ReLU()(self.conv(x) + self.skip(x))\n",
    "\n",
    "class CNN_C(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = ResidualBlock(1,16)\n",
    "        self.pool1  = nn.MaxPool2d(2)\n",
    "        self.layer2 = ResidualBlock(16,32)\n",
    "        self.pool2  = nn.MaxPool2d(2)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(), nn.Linear(32*7*7,64), nn.ReLU(), nn.Linear(64,10)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x = self.pool1(self.layer1(x))\n",
    "        x = self.pool2(self.layer2(x))\n",
    "        return self.classifier(x)\n",
    "\n",
    "```\n",
    "\n",
    "* **Learning Curves:** \n",
    "![CNN-C Loss & Accuracy Curves](./output/cnn_c-model.png)\n",
    "\n",
    "\n",
    "* **Results:** Stop epoch \\~8, Test Acc = 93.61%\n",
    "* **Analysis:** Residual connections speed training; strong performance with moderate capacity.\n",
    "\n",
    "### 5.4 Training Loop with Early Stopping\n",
    "\n",
    "* **Code Placeholder:**\n",
    "\n",
    "```python\n",
    "# train_es function and model instantiation/training loop\n",
    "def train_with_early_stopping(model, loaders, criterion, optimizer, epochs, patience):\n",
    "    best_val = float('inf')\n",
    "    counter = 0\n",
    "    # Added 'train_acc' key\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],    # track train accuracy\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "\n",
    "    print(f\"Training {model.__class__.__name__} with early stopping...\")\n",
    "    for ep in range(1, epochs+1):\n",
    "        # ----- Training Phase -----\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        total_correct = 0\n",
    "        for X, y in loaders['train']:\n",
    "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss    += loss.item() * X.size(0)\n",
    "            total_correct += (outputs.argmax(1) == y).sum().item()\n",
    "\n",
    "        train_loss /= len(loaders['train'].dataset)\n",
    "        train_acc   = total_correct / len(loaders['train'].dataset)\n",
    "\n",
    "        # ----- Validation Phase -----\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct  = 0\n",
    "        with torch.no_grad():\n",
    "            for X, y in loaders['val']:\n",
    "                X, y = X.to(DEVICE), y.to(DEVICE)\n",
    "                outputs = model(X)\n",
    "                val_loss += criterion(outputs, y).item() * X.size(0)\n",
    "                correct  += (outputs.argmax(1) == y).sum().item()\n",
    "\n",
    "        val_loss /= len(loaders['val'].dataset)\n",
    "        val_acc   = correct / len(loaders['val'].dataset)\n",
    "\n",
    "        # ----- Record History -----\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)   # store train accuracy\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "\n",
    "        print(f\"Epoch {ep}: \"\n",
    "            f\"train_loss={train_loss:.4f}, train_acc={train_acc:.4f}, \"\n",
    "            f\"val_loss={val_loss:.4f}, val_acc={val_acc:.4f}\")\n",
    "\n",
    "        # ----- Early Stopping -----\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            counter = 0\n",
    "            torch.save(model.state_dict(), 'best.pt')\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    # Load best checkpoint\n",
    "    model.load_state_dict(torch.load('best.pt'))\n",
    "    return history\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Final Comparative Analysis\n",
    "\n",
    "| Model     | Test Acc  | Epochs | Remarks                                  |\n",
    "| --------- | --------- | ------ | ---------------------------------------- |\n",
    "| Linear    | 68.8%     | 20     | Underfits                                |\n",
    "| MLP40     | 94.8%     | 20     | Overfits; ignores spatial structure      |\n",
    "| SimpleCNN | 94.6%     | 20     | Good baseline                            |\n",
    "| **CNN A** | **95.7%** | 18     | Best: BN + Dropout, balanced depth/width |\n",
    "| CNN C     | 93.6%     | 8      | Fast converge with residual blocks       |\n",
    "| CNN B     | 84.0%     | 23     | Lacks normalization; underperforms       |\n",
    "\n",
    "**General Conclusion:**\n",
    "After evaluating six models of increasing complexity, **CNN_A** emerges as the clear top performer, achieving **95.7%** test accuracy thanks to batch normalization, dropout regularization, and sufficient convolutional depth. Its stable training/validation curves and resistance to overfitting make it the recommended architecture for KMNIST.\n",
    "\n",
    "\n",
    "\n",
    "**Future Improvements:**\n",
    "\n",
    "* **Data Augmentation:** rotations/shifts/elastic to boost invariance.\n",
    "* **LR Scheduling:** step decay or cosine annealing for smoother convergence.\n",
    "* **Deeper Residual Stacks:** extend CNN\\_C for higher capacity.\n",
    "* **Regularization:** weight decay, label smoothing to further reduce overfitting.\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "## 7. Submission Details\n",
    "\n",
    "* **Report:** `hw4_report.pdf`\n",
    "* **Source Code:** `hw4_kmnist.py`\n",
    "* **Requirements:** `requirements.txt`\n",
    "* **Environment:** Python >= 3.12, PyTorch >= 1.12.0, torchvision >= 0.13.0\n",
    "* **Dataset:** KMNIST (downloaded automatically)\n",
    "* **Hardware:** NVIDIA GeForce RTX 3060\n",
    "* **Training Time:** \\~1 hours for all models\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
