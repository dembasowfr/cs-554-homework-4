{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72fb2b76",
   "metadata": {},
   "source": [
    "# CS454-554 Homework 4 Report: KMNIST Classification\n",
    "\n",
    "**Due:** May 14, 2025, 23:00\n",
    "**Author:** **DEMBA SOW**\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "The objective of this assignment is to implement and compare multiple neural network architectures on the KMNIST dataset—a 10-class classification problem of 28×28 grayscale Japanese character images—using PyTorch. We evaluate:\n",
    "\n",
    "1. **Baseline Models**\n",
    "\n",
    "   * Linear\n",
    "   * Multilayer Perceptron (40 hidden units)\n",
    "   * Simple CNN\n",
    "2. **Extended CNN Variants**\n",
    "\n",
    "   * CNN‑A (Deeper + BatchNorm + Dropout)\n",
    "   * CNN‑B (Wider Filters + Global Pooling)\n",
    "   * CNN‑C (Residual Connections)\n",
    "3. **Hyperparameter Tuning & Early Stopping**\n",
    "4. **Comparison & Discussion**\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Environment & Data Loading\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision.datasets import KMNIST\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_ds = KMNIST('./data', train=True,  transform=transform, download=True)\n",
    "test_ds  = KMNIST('./data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64,  shuffle=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=64)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Model Architectures\n",
    "\n",
    "### 3.1. Linear Model\n",
    "\n",
    "* Single `nn.Linear(28×28, 10)` layer\n",
    "\n",
    "```\tpython\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(28*28, 10)\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "```\n",
    "\n",
    "### 3.2. MLP (40 Hidden Units)\n",
    "\n",
    "* Flatten → Linear(784→40) → ReLU → Linear(40→10)\n",
    "```python\n",
    "class MLP40(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28*28, 40),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(40, 10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "```\n",
    "\n",
    "### 3.3. Simple CNN\n",
    "\n",
    "* Conv(1→16,3) → ReLU → MaxPool(2)\n",
    "* Conv(16→32,3) → ReLU → MaxPool(2)\n",
    "* FC(32×7×7→64) → ReLU → FC(64→10)\n",
    "\n",
    "```python\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),  # 16×14×14\n",
    "            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2)  # 32×7×7\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32*7*7, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return self.classifier(x)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Training & Evaluation\n",
    "\n",
    "* **Loss:** `nn.CrossEntropyLoss`\n",
    "* **Optimizer:** `optim.Adam` (LR=0.001)\n",
    "* **Batch size:** 64\n",
    "* **Epochs:** 20 (Baseline), 50 (Extended)\n",
    "* **Early Stopping:** Monitor validation loss, stop if no improvement for 3 epochs\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Baseline Results\n",
    "\n",
    "### 5.1. Accuracy & Loss Curves\n",
    "\n",
    "* **Linear:** \n",
    "![Linear Accuracy & Loss Curves](./output/linear-model.png)\n",
    "\n",
    "* **MLP40:** \n",
    "![MLP40 Accuracy & Loss Curves](./output/mlp-40-model.png)\n",
    "\n",
    "* **Simple CNN:** \n",
    "![Simple CNN Accuracy & Loss Curves](./output/simple-cnn-model.png)\n",
    "\n",
    "\n",
    "### 5.2. Numerical Results\n",
    "\n",
    "| Model      | Epoch | Train Loss | Train Acc | Test Loss | Test Acc |\n",
    "| ---------- | :---: | :--------: | :-------: | :-------: | :------: |\n",
    "| Linear     |   10  |    0.560   |   0.835   |   1.036   |   0.694  |\n",
    "| MLP40      |   10  |    0.159   |   0.954   |   0.545   |   0.847  |\n",
    "| Simple CNN |   10  |    0.016   |   0.995   |   0.307   |   0.940  |\n",
    "\n",
    "\n",
    "> **Observations:**\n",
    ">\n",
    "> * The Linear model underfits, plateauing at \\~69% accuracy.\n",
    "> * MLP40 improves to \\~84.7% by adding non-linearity.\n",
    "> * Simple CNN achieves \\~94%, leveraging spatial features.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Extended CNN Variants\n",
    "\n",
    "### 6.1. Loss & Accuracy Curves\n",
    "\n",
    "* **CNN‑A:** \n",
    "![CNN-A Loss & Accuracy Curves](./output/cnn_a-model.png)\n",
    "\n",
    "* **CNN‑B:**\n",
    "![CNN-B Loss & Accuracy Curves](./output/cnn_b-model.png)\n",
    "\n",
    "* **CNN‑C:** \n",
    "![CNN-C Loss & Accuracy Curves](./output/cnn_c-model.png)\n",
    "\n",
    "### 6.2. Numerical Comparison\n",
    "\n",
    "| Model | Best Epoch | Test Loss | Test Acc   |\n",
    "| ----- | ---------- | --------- | ---------- |\n",
    "| CNN‑A | 14         | \\~0.169   | **0.9566** |\n",
    "| CNN‑B | 7          | \\~0.623   | 0.8206     |\n",
    "| CNN‑C | 6          | \\~0.252   | 0.9347     |\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Discussion & Conclusion\n",
    "\n",
    "Extended CNN models yielded significant performance improvements over the baseline:\n",
    "\n",
    "* **CNN‑A**, with additional depth, BatchNorm, and Dropout, achieved the highest test accuracy of **95.66%**. Its regularization and normalization helped generalization.\n",
    "* **CNN‑B**, despite wider filters and global pooling, plateaued early, likely due to over-smoothing from global pooling.\n",
    "* **CNN‑C** used residual connections and performed strongly at **93.47%**, showing effectiveness of skip connections for stability.\n",
    "\n",
    "Compared to Simple CNN (94%), both CNN‑A and CNN‑C showed improvements, with CNN‑A emerging as the best model.\n",
    "\n",
    "**Best Model: CNN‑A** with hyperparameters (lr=0.0005, batch=128), early stopped at epoch 14.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Submission Details\n",
    "\n",
    "* **Report:** `hw4_report.pdf`\n",
    "* **Source Code:** `hw4_kmnist.py`\n",
    "* **Requirements:** `requirements.txt`\n",
    "* **Environment:** Python >= 3.12, PyTorch >= 1.12.0, torchvision >= 0.13.0\n",
    "* **Dataset:** KMNIST (downloaded automatically)\n",
    "* **Hardware:** NVIDIA GeForce RTX 3060\n",
    "* **Training Time:** \\~1 hours for all models\n",
    "* **GPU Utilization:** 100% during training\n",
    "* **Memory Usage:** \\~4GB\n",
    "* **Batch Size:** 64 for both baseline and extended models\n",
    "* **Epochs:** 20 for baseline, 50 for extended models\n",
    "* **Early Stopping:** Implemented for extended models\n",
    "\n",
    "\n",
    "## CS454-554 Homework 4: KMNIST Classification with PyTorch\n",
    "**Student Name:** Demba Sow\n",
    "**Due:** May 14, 2025, 23:00\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Introduction\n",
    "\n",
    "The goal of this assignment is to implement and compare three neural network architectures of increasing complexity on the KMNIST dataset (10-class Japanese character images, 28×28 grayscale) using PyTorch. We evaluate:\n",
    "\n",
    "1. **Linear model**\n",
    "2. **Multilayer Perceptron** (single hidden layer with 40 neurons)\n",
    "3. **Convolutional Neural Network** (custom CNN)\n",
    "\n",
    "For each architecture, we present:\n",
    "\n",
    "* **Model architecture details**\n",
    "* **Training & test loss curves**\n",
    "* **Training & test accuracy curves**\n",
    "* **Comparative analysis**\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Setup: Data & Utilities\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import KMNIST\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "LR = 1e-3\n",
    "EPOCHS = 20\n",
    "\n",
    "# Data transforms & loaders\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "train_ds = KMNIST('./data', train=True, transform=transform, download=True)\n",
    "test_ds  = KMNIST('./data', train=False,transform=transform, download=True)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE)\n",
    "\n",
    "# Training utilities\n",
    "def train_epoch(model, loader, criterion, optimizer):\n",
    "    model.train(); total_loss=total_correct=0\n",
    "    for X,y in loader:\n",
    "        X,y = X.to(DEVICE), y.to(DEVICE)\n",
    "        preds = model(X)\n",
    "        loss = criterion(preds, y)\n",
    "        optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "        total_loss   += loss.item()*X.size(0)\n",
    "        total_correct+= (preds.argmax(1)==y).sum().item()\n",
    "    n=len(loader.dataset)\n",
    "    return total_loss/n, total_correct/n\n",
    "\n",
    "def eval_model(model, loader, criterion):\n",
    "    model.eval(); total_loss=total_correct=0\n",
    "    with torch.no_grad():\n",
    "        for X,y in loader:\n",
    "            X,y = X.to(DEVICE), y.to(DEVICE)\n",
    "            out = model(X)\n",
    "            total_loss   += criterion(out,y).item()*X.size(0)\n",
    "            total_correct+= (out.argmax(1)==y).sum().item()\n",
    "    n=len(loader.dataset)\n",
    "    return total_loss/n, total_correct/n\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Linear Model\n",
    "\n",
    "#### 3.1 Architecture\n",
    "\n",
    "A single fully-connected layer mapping the flattened 28×28 input to 10 outputs.\n",
    "\n",
    "```python\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(28*28, 10)\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "```\n",
    "\n",
    "#### 3.2 Training\n",
    "\n",
    "```python\n",
    "model = LinearModel().to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "hist_lin = {'train_loss':[], 'train_acc':[], 'test_loss':[], 'test_acc':[]}\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    tl, ta = train_epoch(model, train_loader, criterion, optimizer)\n",
    "    vl, va = eval_model(model, test_loader,   criterion)\n",
    "    hist_lin['train_loss'].append(tl)\n",
    "    hist_lin['train_acc'].append(ta)\n",
    "    hist_lin['test_loss'].append(vl)\n",
    "    hist_lin['test_acc'].append(va)\n",
    "    print(f\"Epoch {epoch}: TL={tl:.3f}, TA={ta:.3f}, VL={vl:.3f}, VA={va:.3f}\")\n",
    "```\n",
    "\n",
    "#### 3.3 Results & Analysis\n",
    "\n",
    "| Epoch | Train Loss | Test Loss | Train Acc | Test Acc |\n",
    "| ----- | ---------- | --------- | --------- | -------- |\n",
    "| 20    | 0.567      | 1.059     | 82.9%     | 68.8%    |\n",
    "\n",
    "* **Analysis:** The linear model underfits: low capacity leads to high bias, with a large gap between train and test accuracy (\\~14%).\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Multilayer Perceptron (MLP40)\n",
    "\n",
    "#### 4.1 Architecture\n",
    "\n",
    "One hidden layer of 40 ReLU units, followed by a 10-way output.\n",
    "\n",
    "```python\n",
    "class MLP40(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28*28, 40), nn.ReLU(),\n",
    "            nn.Linear(40, 10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "```\n",
    "\n",
    "#### 4.2 Training\n",
    "\n",
    "```python\n",
    "model = MLP40().to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "hist_mlp = {'train_loss':[], 'train_acc':[], 'test_loss':[], 'test_acc':[]}\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    tl, ta = train_epoch(model, train_loader, criterion, optimizer)\n",
    "    vl, va = eval_model(model, test_loader,   criterion)\n",
    "    hist_mlp['train_loss'].append(tl)\n",
    "    hist_mlp['train_acc'].append(ta)\n",
    "    hist_mlp['test_loss'].append(vl)\n",
    "    hist_mlp['test_acc'].append(va)\n",
    "    print(f\"Epoch {epoch}: TL={tl:.3f}, TA={ta:.3f}, VL={vl:.3f}, VA={va:.3f}\")\n",
    "```\n",
    "\n",
    "#### 4.3 Results & Analysis\n",
    "\n",
    "| Epoch | Train Loss | Test Loss | Train Acc | Test Acc |\n",
    "| ----- | ---------- | --------- | --------- | -------- |\n",
    "| 20    | 0.006      | 0.450     | 99.8%     | 94.8%    |\n",
    "\n",
    "* **Analysis:** The MLP fits training almost perfectly but shows a \\~5% drop to test accuracy, indicating overfitting. Spatial structure is underutilized.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Convolutional Neural Network (SimpleCNN)\n",
    "\n",
    "#### 5.1 Architecture\n",
    "\n",
    "Two conv→ReLU→pool blocks, then FC(64)→ReLU→FC(10).\n",
    "\n",
    "```python\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1,16,3,padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16,32,3,padding=1), nn.ReLU(), nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32*7*7,64), nn.ReLU(),\n",
    "            nn.Linear(64,10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return self.classifier(x)\n",
    "```\n",
    "\n",
    "#### 5.2 Training\n",
    "\n",
    "```python\n",
    "model = SimpleCNN().to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "hist_cnn = {'train_loss':[], 'train_acc':[], 'test_loss':[], 'test_acc':[]}\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    tl, ta = train_epoch(model, train_loader, criterion, optimizer)\n",
    "    vl, va = eval_model(model, test_loader,   criterion)\n",
    "    hist_cnn['train_loss'].append(tl)\n",
    "    hist_cnn['train_acc'].append(ta)\n",
    "    hist_cnn['test_loss'].append(vl)\n",
    "    hist_cnn['test_acc'].append(va)\n",
    "    print(f\"Epoch {epoch}: TL={tl:.3f}, TA={ta:.3f}, VL={vl:.3f}, VA={va:.3f}\")\n",
    "```\n",
    "\n",
    "#### 5.3 Results & Analysis\n",
    "\n",
    "| Epoch | Train Loss | Test Loss | Train Acc | Test Acc |\n",
    "| ----- | ---------- | --------- | --------- | -------- |\n",
    "| 20    | 0.001      | 0.527     | 99.9%     | 94.6%    |\n",
    "\n",
    "* **Analysis:** SimpleCNN captures spatial features, matching MLP40’s test accuracy but with more stable generalization and reduced overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Comparative Analysis\n",
    "\n",
    "| Model     | Test Acc | Remarks                                 |\n",
    "| --------- | -------- | --------------------------------------- |\n",
    "| Linear    | 68.8%    | Underfits—insufficient capacity         |\n",
    "| MLP40     | 94.8%    | Overfits—lacks spatial inductive bias   |\n",
    "| SimpleCNN | 94.6%    | Balanced—leverages convolutional layers |\n",
    "\n",
    "**Conclusion:** The convolutional architecture provides the best trade‑off between capacity and generalization on KMNIST. Future work can explore deeper CNNs, regularization, and data augmentation for further gains.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Extended CNN Variants\n",
    "\n",
    "We now explore three deeper CNNs: `CNN_A`, `CNN_B`, and `CNN_C`—all trained with early stopping (patience=3), lr=5e-4, bs=128, max epochs=50.\n",
    "\n",
    "#### 6.1 Architectures\n",
    "\n",
    "```python\n",
    "class CNN_A(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1,32,3,padding=1), nn.BatchNorm2d(32), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32,64,3,padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
    "            nn.MaxPool2d(2), nn.Dropout(0.25)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(), nn.Linear(64*7*7,128), nn.ReLU(), nn.Dropout(0.5),\n",
    "            nn.Linear(128,10)\n",
    "        )\n",
    "    def forward(self,x): return self.classifier(self.features(x))\n",
    "\n",
    "class CNN_B(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1,16,5,padding=2), nn.ReLU(),\n",
    "            nn.Conv2d(16,32,5,padding=2), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32,64,3,padding=1), nn.ReLU(), nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "        self.fc = nn.Linear(64,10)\n",
    "    def forward(self,x): return self.fc(self.features(x).view(x.size(0),-1))\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self,in_ch,out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch,out_ch,3,padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(out_ch,out_ch,3,padding=1)\n",
    "        )\n",
    "        self.skip = nn.Conv2d(in_ch,out_ch,1) if in_ch!=out_ch else nn.Identity()\n",
    "    def forward(self,x): return nn.ReLU()(self.conv(x)+self.skip(x))\n",
    "\n",
    "class CNN_C(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = ResidualBlock(1,16)\n",
    "        self.pool1  = nn.MaxPool2d(2)\n",
    "        self.layer2 = ResidualBlock(16,32)\n",
    "        self.pool2  = nn.MaxPool2d(2)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(), nn.Linear(32*7*7,64), nn.ReLU(), nn.Linear(64,10)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x = self.pool1(self.layer1(x))\n",
    "        x = self.pool2(self.layer2(x))\n",
    "        return self.classifier(x)\n",
    "```\n",
    "\n",
    "#### 6.2 Training with Early Stopping\n",
    "\n",
    "```python\n",
    "# Early-stopping loop\n",
    "def train_es(model, train_dl, val_dl, criterion, optimizer, max_epochs=50, patience=3):\n",
    "    best_val=float('inf'); counter=0; history={'train_loss':[],'val_loss':[],'val_acc':[]}\n",
    "    for ep in range(1,max_epochs+1):\n",
    "        # train\n",
    "        model.train(); tloss=0\n",
    "        for X,y in train_dl:\n",
    "            X,y = X.to(DEVICE), y.to(DEVICE)\n",
    "            optimizer.zero_grad(); loss=criterion(model(X),y)\n",
    "            loss.backward(); optimizer.step()\n",
    "            tloss += loss.item()*X.size(0)\n",
    "        tloss/=len(train_dl.dataset)\n",
    "        # val\n",
    "        model.eval(); vloss=0; correct=0\n",
    "        with torch.no_grad():\n",
    "            for X,y in val_dl:\n",
    "                X,y = X.to(DEVICE), y.to(DEVICE)\n",
    "                out=model(X)\n",
    "                vloss += criterion(out,y).item()*X.size(0)\n",
    "                correct += (out.argmax(1)==y).sum().item()\n",
    "        vloss/=len(val_dl.dataset); vacc=correct/len(val_dl.dataset)\n",
    "        history['train_loss'].append(tloss); history['val_loss'].append(vloss); history['val_acc'].append(vacc)\n",
    "        print(f\"Epoch {ep}: train_loss={tloss:.4f}, val_loss={vloss:.4f}, val_acc={vacc:.4f}\")\n",
    "        if vloss<best_val:\n",
    "            best_val=vloss; counter=0; torch.save(model.state_dict(),'best.pt')\n",
    "        else:\n",
    "            counter+=1; \n",
    "            if counter>=patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "    model.load_state_dict(torch.load('best.pt'))\n",
    "    return history\n",
    "\n",
    "# Instantiate and train\n",
    "from torch.utils.data import DataLoader\n",
    "train_dl = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
    "val_dl   = DataLoader(test_ds,  batch_size=128)\n",
    "hist_ext={}\n",
    "best={'model':None,'acc':0}\n",
    "for cls in (CNN_A,CNN_B,CNN_C):\n",
    "    mdl=cls().to(DEVICE)\n",
    "    opt=optim.Adam(mdl.parameters(),lr=5e-4)\n",
    "    h=train_es(mdl,train_dl,val_dl,nn.CrossEntropyLoss(),opt)\n",
    "    _,acc = eval_model(mdl,test_loader,nn.CrossEntropyLoss())\n",
    "    hist_ext[cls.__name__]=h\n",
    "    print(f\"{cls.__name__} final test acc={acc:.4f}\")\n",
    "    if acc>best['acc']: best.update({'model':cls.__name__,'acc':acc})\n",
    "print(f\"Best extended model: {best['model']} with acc={best['acc']:.4f}\")\n",
    "```\n",
    "\n",
    "#### 6.3 Results & Analysis\n",
    "\n",
    "| Model  | Stop Epoch | Test Acc |\n",
    "| ------ | ---------- | -------- |\n",
    "| CNN\\_A | 18         | 95.68%   |\n",
    "| CNN\\_B | 23         | 84.02%   |\n",
    "| CNN\\_C | 8          | 93.61%   |\n",
    "\n",
    "* **CNN\\_A**: Best generalization (95.7%), stable loss, effective BN+Dropout.\n",
    "* **CNN\\_B**: Limited by missing normalization; underperforms.\n",
    "* **CNN\\_C**: Fast converge via residuals, strong performance (93.6%).\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Final Comparative Analysis\n",
    "\n",
    "| Model      | Test Acc  | Notes                               |\n",
    "| ---------- | --------- | ----------------------------------- |\n",
    "| Linear     | 68.8%     | Underfits                           |\n",
    "| MLP40      | 94.8%     | Overfits; ignores spatial structure |\n",
    "| SimpleCNN  | 94.6%     | Good baseline                       |\n",
    "| **CNN\\_A** | **95.7%** | Top performer with BN & Dropout     |\n",
    "| CNN\\_C     | 93.6%     | Fast converge with residual blocks  |\n",
    "| CNN\\_B     | 84.0%     | Lacks normalization; lower capacity |\n",
    "\n",
    "**Conclusion:** `CNN_A` is recommended for KMNIST based on highest accuracy and robustness. Future steps: data augmentation, LR schedules, deeper residual stacks.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Plots\n",
    "\n",
    "```python\n",
    "# Plot extended histories\n",
    "def plot_extended(hist,name):\n",
    "    epochs=range(1,len(hist['train_loss'])+1)\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(epochs,hist['train_loss'],label='Train Loss')\n",
    "    plt.plot(epochs,hist['val_loss'],label='Val Loss')\n",
    "    plt.title(f'{name} Loss'); plt.xlabel('Epoch'); plt.legend()\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(epochs,hist['val_acc'],label='Val Acc')\n",
    "    plt.title(f'{name} Val Acc'); plt.xlabel('Epoch'); plt.legend()\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "for name,h in hist_ext.items(): plot_extended(h,name)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 9. Overall Conclusion & Future Improvements\n",
    "After evaluating six models of increasing complexity, **CNN_A** emerges as the clear top performer, achieving **95.7%** test accuracy thanks to batch normalization, dropout regularization, and sufficient convolutional depth. Its stable training/validation curves and resistance to overfitting make it the recommended architecture for KMNIST.\n",
    "\n",
    "**Why CNN_A Performs Best:**\n",
    "- **BatchNorm** accelerates convergence and smooths the loss landscape.  \n",
    "- **Dropout** combats overfitting, yielding tighter train–test performance.  \n",
    "- **Balanced Depth & Width** offers enough capacity for complex patterns without excessive parameters.\n",
    "\n",
    "**Room for Improvement:**\n",
    "- **Data Augmentation:** introducing random rotations, shifts, or elastic distortions could further generalize performance.  \n",
    "- **Learning-Rate Scheduling:** using cosine annealing or step decay may optimize convergence.  \n",
    "- **Deeper Residual Networks:** stacking more residual blocks (as in CNN_C) could push accuracy even higher with minimal training overhead.  \n",
    "- **Weight Decay & Regularization:** applying L2 penalty or label smoothing could refine generalization.\n",
    "\n",
    "---\n",
    "\n",
    "*End of Assignment Report.*\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
