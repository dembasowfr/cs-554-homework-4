{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72fb2b76",
   "metadata": {},
   "source": [
    "# CS454-554 Homework 4 Report: KMNIST Classification\n",
    "\n",
    "**Due:** May 14, 2025, 23:00\n",
    "**Author:** **DEMBA SOW**\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "The objective of this assignment is to implement and compare multiple neural network architectures on the KMNIST dataset—a 10-class classification problem of 28×28 grayscale Japanese character images—using PyTorch. We evaluate:\n",
    "\n",
    "1. **Baseline Models**\n",
    "\n",
    "   * Linear\n",
    "   * Multilayer Perceptron (40 hidden units)\n",
    "   * Simple CNN\n",
    "2. **Extended CNN Variants**\n",
    "\n",
    "   * CNN‑A (Deeper + BatchNorm + Dropout)\n",
    "   * CNN‑B (Wider Filters + Global Pooling)\n",
    "   * CNN‑C (Residual Connections)\n",
    "3. **Hyperparameter Tuning & Early Stopping**\n",
    "4. **Comparison & Discussion**\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Environment & Data Loading\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision.datasets import KMNIST\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_ds = KMNIST('./data', train=True,  transform=transform, download=True)\n",
    "test_ds  = KMNIST('./data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64,  shuffle=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=64)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Model Architectures\n",
    "\n",
    "### 3.1. Linear Model\n",
    "\n",
    "* Single `nn.Linear(28×28, 10)` layer\n",
    "\n",
    "```\tpython\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(28*28, 10)\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "```\n",
    "\n",
    "### 3.2. MLP (40 Hidden Units)\n",
    "\n",
    "* Flatten → Linear(784→40) → ReLU → Linear(40→10)\n",
    "```python\n",
    "class MLP40(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28*28, 40),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(40, 10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "```\n",
    "\n",
    "### 3.3. Simple CNN\n",
    "\n",
    "* Conv(1→16,3) → ReLU → MaxPool(2)\n",
    "* Conv(16→32,3) → ReLU → MaxPool(2)\n",
    "* FC(32×7×7→64) → ReLU → FC(64→10)\n",
    "\n",
    "```python\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),  # 16×14×14\n",
    "            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2)  # 32×7×7\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32*7*7, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return self.classifier(x)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Training & Evaluation\n",
    "\n",
    "* **Loss:** `nn.CrossEntropyLoss`\n",
    "* **Optimizer:** `optim.Adam` (LR=0.001)\n",
    "* **Batch size:** 64\n",
    "* **Epochs:** 20 (Baseline), 50 (Extended)\n",
    "* **Early Stopping:** Monitor validation loss, stop if no improvement for 3 epochs\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Baseline Results\n",
    "\n",
    "### 5.1. Accuracy & Loss Curves\n",
    "\n",
    "* **Linear:** \n",
    "![Linear Accuracy & Loss Curves](./output/linear-model.png)\n",
    "\n",
    "* **MLP40:** \n",
    "![MLP40 Accuracy & Loss Curves](./output/mlp-40-model.png)\n",
    "\n",
    "* **Simple CNN:** \n",
    "![Simple CNN Accuracy & Loss Curves](./output/simple-cnn-model.png)\n",
    "\n",
    "\n",
    "### 5.2. Numerical Results\n",
    "\n",
    "| Model      | Epoch | Train Loss | Train Acc | Test Loss | Test Acc |\n",
    "| ---------- | :---: | :--------: | :-------: | :-------: | :------: |\n",
    "| Linear     |   10  |    0.560   |   0.835   |   1.036   |   0.694  |\n",
    "| MLP40      |   10  |    0.159   |   0.954   |   0.545   |   0.847  |\n",
    "| Simple CNN |   10  |    0.016   |   0.995   |   0.307   |   0.940  |\n",
    "\n",
    "\n",
    "> **Observations:**\n",
    ">\n",
    "> * The Linear model underfits, plateauing at \\~69% accuracy.\n",
    "> * MLP40 improves to \\~84.7% by adding non-linearity.\n",
    "> * Simple CNN achieves \\~94%, leveraging spatial features.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Extended CNN Variants\n",
    "\n",
    "### 6.1. Loss & Accuracy Curves\n",
    "\n",
    "* **CNN‑A:** \n",
    "![CNN-A Loss & Accuracy Curves](./output/cnn-a-model.png)\n",
    "\n",
    "* **CNN‑B:**\n",
    "![CNN-B Loss & Accuracy Curves](./output/cnn-b-model.png)\n",
    "\n",
    "* **CNN‑C:** \n",
    "![CNN-C Loss & Accuracy Curves](./output/cnn-c-model.png)\n",
    "\n",
    "### 6.2. Numerical Comparison\n",
    "\n",
    "| Model | Best Epoch | Test Loss | Test Acc   |\n",
    "| ----- | ---------- | --------- | ---------- |\n",
    "| CNN‑A | 14         | \\~0.169   | **0.9566** |\n",
    "| CNN‑B | 7          | \\~0.623   | 0.8206     |\n",
    "| CNN‑C | 6          | \\~0.252   | 0.9347     |\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Discussion & Conclusion\n",
    "\n",
    "Extended CNN models yielded significant performance improvements over the baseline:\n",
    "\n",
    "* **CNN‑A**, with additional depth, BatchNorm, and Dropout, achieved the highest test accuracy of **95.66%**. Its regularization and normalization helped generalization.\n",
    "* **CNN‑B**, despite wider filters and global pooling, plateaued early, likely due to over-smoothing from global pooling.\n",
    "* **CNN‑C** used residual connections and performed strongly at **93.47%**, showing effectiveness of skip connections for stability.\n",
    "\n",
    "Compared to Simple CNN (94%), both CNN‑A and CNN‑C showed improvements, with CNN‑A emerging as the best model.\n",
    "\n",
    "**Best Model: CNN‑A** with hyperparameters (lr=0.0005, batch=128), early stopped at epoch 14.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Submission Details\n",
    "\n",
    "* **Report:** `hw4_report.pdf`\n",
    "* **Source Code:** `hw4_kmnist.py`\n",
    "* **Requirements:** `requirements.txt`\n",
    "* **Environment:** Python >= 3.12, PyTorch >= 1.12.0, torchvision >= 0.13.0\n",
    "* **Dataset:** KMNIST (downloaded automatically)\n",
    "* **Hardware:** NVIDIA GeForce RTX 3060\n",
    "* **Training Time:** \\~1 hours for all models\n",
    "* **GPU Utilization:** 100% during training\n",
    "* **Memory Usage:** \\~4GB\n",
    "* **Batch Size:** 64 for both baseline and extended models\n",
    "* **Epochs:** 20 for baseline, 50 for extended models\n",
    "* **Early Stopping:** Implemented for extended models\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
